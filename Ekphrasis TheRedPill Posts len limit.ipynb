{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "# from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns \n",
    "import scipy \n",
    "import statistics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origDataAddr = 'data/original/TheRedPill/'\n",
    "outAddr = 'data/preprocessed/TheRedPill/'\n",
    "\n",
    "if not os.path.exists(outAddr):\n",
    "    os.makedirs(outAddr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tableNames = [\n",
    "    \"`fh-bigquery.reddit_posts.2015_12`\"\n",
    "]\n",
    "\n",
    "for year in range(2016, 2017+1):\n",
    "    for month in range(1,12+1):\n",
    "        month = \"{0:0=2d}\".format(month)\n",
    "        tableNames.append(\"`fh-bigquery.reddit_posts.\"+str(year)+\"_\"+str(month)+\"`\")\n",
    "        \n",
    "for year in range(2018, 2018+1):\n",
    "    for month in range(1, 12+1):\n",
    "        month = \"{0:0=2d}\".format(month)\n",
    "        tableNames.append(\"`fh-bigquery.reddit_posts.\"+str(year)+\"_\"+str(month)+\"`\")\n",
    "        \n",
    "tableNames += [\n",
    "     '`fh-bigquery.reddit_posts.2019_01`'\n",
    "]\n",
    "\n",
    "tableNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read reddit  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfDict = {}\n",
    "for tableName in tableNames:\n",
    "    print (tableName)\n",
    "    filename = tableName[1:-1]\n",
    "    try:\n",
    "        df = pd.read_csv(origDataAddr + filename + '.csv', index_col=0)\n",
    "    except:\n",
    "        print ('Error')\n",
    "        continue\n",
    "    print (len(df))\n",
    "    dfDict[filename] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfDict.values()).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use only relevant columns, replace name column with t3_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tablename in dfDict:\n",
    "    print (tablename)\n",
    "    cols = [ 'name', 'id', 'title', 'selftext' ]\n",
    "    df = dfDict[tablename]\n",
    "    df = df[cols]\n",
    "    df['name'] = 't3_'+df['id']\n",
    "    dfDict[tablename] = df\n",
    "    print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfDict.values()).reset_index(drop=True)\n",
    "print (len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested text prep flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess title, selftext, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tablename in dfDict:\n",
    "    print (tablename)\n",
    "    \n",
    "    start = time.time()\n",
    "    df = dfDict[tablename]\n",
    "    print (len(df))\n",
    "    df = df[~df['name'].isnull()]\n",
    "    print ('After dropping nan names have ', len(df))\n",
    "    df = df.drop_duplicates(subset=['name'], keep='first')\n",
    "    print ('After dropping duplicate names ', len(df))\n",
    "    \n",
    "    #replace np.nan with '', removed, deleeted as well\n",
    "    df['title_ekphrasis'] = df['title'].replace(np.nan, '', regex=True)\n",
    "    df['selftext_ekphrasis'] = df['selftext'].replace(np.nan, '', regex=True)\n",
    "    print ('After replaceing nans texts with emptry string ')\n",
    "\n",
    "    #fasttext\n",
    "    df['title_ekphrasis'] = df['title_ekphrasis'].apply(lambda x:text_processor.pre_process_doc(x))\n",
    "    df['title_ekphrasis'] = df['title_ekphrasis'].replace(np.nan, '', regex=True)\n",
    "    \n",
    "    print ('After title prep')\n",
    "    df['selftext_ekphrasis'] = df['selftext_ekphrasis'].apply(lambda x:text_processor.pre_process_doc(x))\n",
    "    df['selftext_ekphrasis'] = df['selftext_ekphrasis'].replace(np.nan, '', regex=True)\n",
    "    \n",
    "    print ('After selef text prep')\n",
    "    \n",
    "    dfDict[tablename] = df\n",
    "#     df.to_csv(outAddr + tablename + '.csv')\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time ', end - start)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfDict.values()).reset_index(drop=True)\n",
    "print (len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find list of hashtags used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findOccurrences(s, ch):\n",
    "    return [i for i, letter in enumerate(s) if letter == ch]\n",
    "\n",
    "hashtags = {}\n",
    "for tablename in dfDict:\n",
    "    df = dfDict[tablename]\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['selftext']\n",
    "        if type(text) == str:\n",
    "            finds = findOccurrences(text, '#')\n",
    "        else:\n",
    "            continue\n",
    "        for find in finds:\n",
    "            temp = text[find : ]\n",
    "            tokens = temp.split()\n",
    "            if len(tokens) > 1:\n",
    "                print (tokens[0], tokens[1])\n",
    "            else:\n",
    "                print (tokens[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove certain things in tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanEkphrasis(sample):\n",
    "    sample = ' '.join(sample)\n",
    "    \n",
    "    sample = sample.replace('<allcaps>', ' ')\n",
    "    sample = sample.replace('</allcaps>', ' ')\n",
    "    sample = sample.replace('<repeated>', ' ')\n",
    "    sample = sample.replace('<elongated>', ' ')\n",
    "    sample = sample.replace('<emphasis>', ' ')\n",
    "    sample = sample.replace('<url>', ' ')\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = [ 'name', 'title_preprocessed', 'selftext_preprocessed', 'title_ekphrasis', 'selftext_ekphrasis' ]\n",
    "for tablename in dfDict:\n",
    "    print (tablename)\n",
    "    \n",
    "    start = time.time()\n",
    "    df = dfDict[tablename]\n",
    "    print (len(df))\n",
    "    \n",
    "    #replace np.nan with '', removed, deleeted as well\n",
    "    df['title_preprocessed'] = df['title'].replace(np.nan, '', regex=True)\n",
    "    df['selftext_preprocessed'] = df['selftext'].replace(np.nan, '', regex=True)\n",
    "\n",
    "    df['title_preprocessed'] = df['title_ekphrasis'].apply(lambda x:cleanEkphrasis(x))\n",
    "    df['title_preprocessed'] = df['title_preprocessed'].replace(np.nan, '', regex=True)\n",
    "    \n",
    "    print ('After title prep')\n",
    "    df['selftext_preprocessed'] = df['selftext_ekphrasis'].apply(lambda x:cleanEkphrasis(x))\n",
    "    df['selftext_preprocessed'] = df['selftext_preprocessed'].replace(np.nan, '', regex=True)\n",
    "    \n",
    "    print ('After self text prep')\n",
    "    \n",
    "    dfDict[tablename] = df\n",
    "#     df[cols].to_csv(outAddr + tablename + '.csv') #to save space on server, only save relevant columns\n",
    "#     df.to_csv(outAddr + tablename + '.csv')\n",
    "\n",
    "    end = time.time()\n",
    "    print('Time ', end - start)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfDict.values()).reset_index(drop=True)\n",
    "print (len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average length distribution of posts before len limiting, after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_preprocessed'] = df['title_preprocessed'] + ' ' + df['selftext_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [ len(i) for i in df['text_preprocessed'] ]\n",
    "print (len(x))\n",
    "print ('Mode ', statistics.mode(x))\n",
    "print ('Median ', statistics.median(x))\n",
    "print (scipy.stats.describe(x))\n",
    "plt.title('TheRedPill length distribution before len limit after clean')\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heuristics for better text classification\n",
    "1. Impose len limits. Drop social data posts if !(len>=256 and len <= 4096) characters the body text (not including the title) was less than 256 or more than 4096 characters in length, AFTER cleaning\n",
    "2. Drop those without selftext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfDict.values()).reset_index(drop=True)\n",
    "print (len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('before dropping those whose selftext is None ', len(df))\n",
    "df = df.dropna(subset=['selftext'])\n",
    "print ('After dropping those whose selftext is None ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Dropping all those without names ', len(df))\n",
    "df = df.dropna(subset=['name'])\n",
    "print ('After ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Dropping all those with duplicate names ', len(df))\n",
    "df = df.drop_duplicates(subset=['name'], keep='first')\n",
    "print ('After ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len_selftext_preprocessed'] = df['selftext_preprocessed'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('before dropping coz too long selftext ', len(df))\n",
    "len_controlled_df = df[ df['len_selftext_preprocessed'] <= 4096 ]\n",
    "len_controlled_df = len_controlled_df.reset_index(drop=True)\n",
    "print ('after dropping ', len(len_controlled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('before dropping coz too short ', len(len_controlled_df))\n",
    "len_controlled_df = len_controlled_df[ len_controlled_df['len_selftext_preprocessed'] >= 256 ]\n",
    "len_controlled_df = len_controlled_df.reset_index(drop=True)\n",
    "print ('after dropping ', len(len_controlled_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine title, selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_controlled_df['text_preprocessed'] = len_controlled_df['title_preprocessed'] + ' ' + len_controlled_df['selftext_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(len_controlled_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average length distribution of posts after len limiting, after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [ len(i) for i in len_controlled_df['text_preprocessed'] ]\n",
    "print (len(x))\n",
    "print ('Mode ', statistics.mode(x))\n",
    "print ('Median ', statistics.median(x))\n",
    "print (scipy.stats.describe(x))\n",
    "plt.title('TheRedPill length distribution after len limit after clean')\n",
    "sns.distplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = len_controlled_df\n",
    "print (len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(outAddr + 'TheRedPillCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outAddr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_preprocessed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
