{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import datetime as dt\n",
    "from scipy import stats\n",
    "import nltk\n",
    "import collections\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from pandas import ExcelWriter\n",
    "import copy\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_xls(filename, list_dfs, tab_names):\n",
    "    writer = ExcelWriter(filename, engine='xlsxwriter', options={'strings_to_urls': False})\n",
    "    for tn, df in zip(tab_names, list_dfs):\n",
    "#         tn = tn[ : tn.find(',')]\n",
    "#         tn = tn.replace(':', ' ')\n",
    "#         print (tn)\n",
    "        df.to_excel(writer, tn, encoding='utf8', header=True,index=False)\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read TheRedPill data\n",
    "1. fasttext preppred\n",
    "2. len limited\n",
    "3. repeated threads removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preppedDataAddr = ''\n",
    "TheRedPill = pd.read_csv(preppedDataAddr + 'TheRedPillCleaned.csv', index_col=0)\n",
    "TheRedPill = TheRedPill.reset_index(drop=True)\n",
    "print (len(TheRedPill))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataDf = TheRedPill.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf = TheRedPill.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem, stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manualStopListAddr = '/home/a383khan/Thesis/Feminism-old-doNotDeleteHasFasttext&ControlUsedForLexiconCreation/hateSpeech/mai/data/'\n",
    "# with open(manualStopListAddr+'manualStopAfterStemList.txt', 'r') as f:\n",
    "#     manual_stop_words_after_stem = f.readlines()\n",
    "#     manual_stop_words_after_stem = list(manual_stop_words_after_stem)\n",
    "#     manual_stop_words_after_stem = [ i.strip() for i in manual_stop_words_after_stem ]\n",
    "\n",
    "my_stop_before_stem = [\n",
    "    'comment', \n",
    "    'endofcomment', \n",
    "    'endoftitlecontent', \n",
    "    'gt', \n",
    "    'mod',\n",
    "    'moderator', \n",
    "    'post', \n",
    "    'subreddit'\n",
    "]\n",
    "stop_words_before_stem = stopwords.words('english')\n",
    "stop_words_before_stem += my_stop_before_stem\n",
    "\n",
    "stop_words_after_stem = []\n",
    "ls = nltk.stem.SnowballStemmer(language='english')\n",
    "for sw in stop_words_before_stem: #stemming even orig stop words so I don't have to run 2 passes for stop word removal\n",
    "    stop_words_after_stem.append(ls.stem(sw))\n",
    "stop_words_after_stem = set(stop_words_after_stem)\n",
    "stop_words_after_stem = list(stop_words_after_stem)\n",
    "\n",
    "# stop_words_after_stem += manual_stop_words_after_stem\n",
    "\n",
    "print (len(stop_words_before_stem), len(stop_words_after_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unidecode\n",
    "import unicodedata\n",
    "\n",
    "def preprocess(text, remove_stop=False, stem=False, remove_nonNoun=False):\n",
    "    \"\"\"\n",
    "    Since we are interested in topics, we removed stopwords and tokens with\n",
    "    fewer than two letters, and we optionally only retain nouns which appear in the WordNet\n",
    "    corpus\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if len(token) > 1] #same as unigrams\n",
    "\n",
    "    if remove_nonNoun is True:\n",
    "        tokens = [ token for token in tokens if token in nouns ]\n",
    "        \n",
    "#     if remove_stop is True:\n",
    "# #         stop_words = stopwords.words('english')\n",
    "# #         stop_words.append('endoftitlecontent'); stop_words.append('endofcomment'); stop_words.append('gt'); \n",
    "#         tokens = [word for word in tokens if word not in stop_words_before_stem]\n",
    "    \n",
    "    if stem is True:\n",
    "        ls = SnowballStemmer(language='english')\n",
    "        tokens = [ls.stem(word) for word in tokens]\n",
    "        \n",
    "    if remove_stop is True: #after stemming removing stop coz I dont know what the original forms were of my custom stop words\n",
    "#         stop_words = stopwords.words('english')\n",
    "#         stop_words.append('endoftitlecontent'); stop_words.append('endofcomment'); stop_words.append('gt'); \n",
    "        tokens = [word for word in tokens if word not in stop_words_after_stem]\n",
    "    \n",
    "        \n",
    "    ret = ' '.join(tokens)\n",
    "    ret = ret.replace('`', ' ')\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf['text_repreprocessed'] = \\\n",
    "    dataDf['text_preprocessed'].apply(lambda x : preprocess(x, remove_stop=True, stem=True, remove_nonNoun=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(dataDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outAddrRoot = 'data/topicModeling/nmf/standardStopRemoved/TheRedPill/'\n",
    "if (not os.path.exists(outAddrRoot)):\n",
    "    os.makedirs(outAddrRoot)\n",
    "dataDf[['text_repreprocessed']].to_csv(outAddrRoot+'TheRedPillRepreprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def getTopicTopWords(model, feature_names, n_top_words):\n",
    "    topicIdToTopWords = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        topicIdToTopWords[topic_idx] = message\n",
    "#         print(message)\n",
    "#     print()\n",
    "    return topicIdToTopWords\n",
    "\n",
    "def getDocIdToTopicId(nmf, tf, dataDf):\n",
    "    print ('\\nTopic assigned to each document in nmf model ')\n",
    "    docIdToTopicId = {}\n",
    "    doc_topic = nmf.transform(tf)\n",
    "    for n in range(doc_topic.shape[0]): #for each document\n",
    "        topic_most_pr = doc_topic[n].argmax() #get the name of the topic that has the highest percentage to it \n",
    "        docIdToTopicId[dataDf.loc[n]['name']] = topic_most_pr\n",
    "\n",
    "    return docIdToTopicId\n",
    "\n",
    "\n",
    "def nmfStuff(df, n_features=1000, n_topics=15, n_top_words=20, \n",
    "             ngram_range_lower=1, ngram_range_upper=1):\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    # data_samples = dataDf['text_preprocessed'].tolist()\n",
    "    data_samples = df['text_repreprocessed'].tolist()\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for NMF.\n",
    "    print(\"Extracting tfidf features for nmf...\")\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                                       max_features=n_features, \n",
    "                                      ngram_range = (ngram_range_lower, ngram_range_upper),\n",
    "                                       stop_words=None)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"Fitting nmf models with tfidf features \")\n",
    "    \n",
    "    nmf = NMF(n_components=n_topics, \n",
    "              random_state=1, alpha=.1, \n",
    "              l1_ratio=.5, init='nndsvd')\n",
    "    nmf.fit(tfidf)\n",
    "    \n",
    "\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    # print_top_words(nmf, tf_feature_names, n_top_words)\n",
    "    topicIdToTopWords = getTopicTopWords(nmf, tfidf_feature_names, n_top_words)\n",
    "    docIdToTopicId = getDocIdToTopicId(nmf, tfidf, df)\n",
    "\n",
    "    return topicIdToTopWords, docIdToTopicId, nmf, tfidf, tfidf_vectorizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Find best n_topics NMF TheRedPill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = dataDf.copy()\n",
    "k_values = [ 10, 50, 100, 200, 300, 400 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# n_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_topics = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = dataDf.copy()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicIdToTopWords, docIdToTopicId, nmf, tf, tf_vectorizer = nmfStuff(df, n_features=1000, \n",
    "    n_topics=n_topics, \n",
    "    n_top_words=30, \n",
    " ngram_range_lower=1, ngram_range_upper=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docIdToTopicId = {} #Ordered so that first is best choice, second is second best choice\n",
    "doc_topic = nmf.transform(tf)\n",
    "for n in range(doc_topic.shape[0]): #for each document\n",
    "    temp = list(doc_topic[n])[:]\n",
    "    \n",
    "    topic_most_pr = np.argmax(temp) #get the name of the topic that has the highest percentage to it \n",
    "    pr = np.max(temp)\n",
    "    docIdToTopicId[df.loc[n]['name']] = [ topic_most_pr ]\n",
    "    temp.pop(topic_most_pr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### How many docs were assigned no, one, and two topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print ('Findings docs assigned one or more topics ')\n",
    "two = 0\n",
    "one = 0\n",
    "for docId in docIdToTopicId:\n",
    "#     print (docIdToTopicId[docId])\n",
    "#     print (type(docIdToTopicId[docId]))\n",
    "    if len(docIdToTopicId[docId]) == 1:\n",
    "        one += 1\n",
    "    elif len(docIdToTopicId[docId]) == 2:\n",
    "        two += 1\n",
    "    else:\n",
    "        print ('Error')\n",
    "\n",
    "print ('Total ', len(df), len(df)/len(dataDf))\n",
    "print ('Zero ', (len(df) - len(docIdToTopicId)), (len(df) - len(docIdToTopicId))/len(df))\n",
    "print ('One ', one, one/len(df))\n",
    "print ('Two ', two, two/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Convert docIdToTopicId to topicIdToDocId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topicIdToDocId = {}\n",
    "for docId in docIdToTopicId:\n",
    "    topicIds = docIdToTopicId[docId]\n",
    "    \n",
    "    topicId = topicIds[0]\n",
    "    if topicId in topicIdToDocId:\n",
    "        topicIdToDocId[topicId].append(docId)\n",
    "    else:\n",
    "        topicIdToDocId[topicId] = []\n",
    "    \n",
    "    if len(topicIds) == 2:\n",
    "        topicId = topicIds[1] #comment this section if you only want to consider the topmost choice of topic for a doc to be valid\n",
    "        if topicId in topicIdToDocId:\n",
    "            topicIdToDocId[topicId].append(docId)\n",
    "        else:\n",
    "            topicIdToDocId[topicId] = []\n",
    "        \n",
    "        \n",
    "print (topicIdToDocId.keys())\n",
    "print (len(topicIdToDocId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Save documents corresponding to each topic as csvs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topicIdToDocIdDfList = []\n",
    "tabNames = []\n",
    "topicIds = list(topicIdToDocId.keys())\n",
    "topicIds.sort()\n",
    "for topicId in topicIds:\n",
    "    print (topicId)\n",
    "    tabNames.append(str(topicId))\n",
    "    topicIdToDocIdDf = df[df['name'].isin(topicIdToDocId[topicId])]\n",
    "    \n",
    "    cols = ['name', 'title', 'selftext', 'text_repreprocessed']\n",
    "    topicIdToDocIdDf = topicIdToDocIdDf[cols]\n",
    "\n",
    "    #add another row at top with only value in name column = actual topic (limited at top 20 words)\n",
    "    footnote = pd.DataFrame([[topicIdToTopWords[topicId], '', '', '']], \n",
    "                            columns=list(topicIdToDocIdDf.columns))\n",
    "    topicIdToDocIdDf = pd.concat([footnote, topicIdToDocIdDf])\n",
    "    topicIdToDocIdDfList.append(topicIdToDocIdDf)\n",
    "#     break\n",
    "    \n",
    "#save this as xlsx file to outAddr\n",
    "filename = 'unigram'+str(n_topics)+'TopicsTopicIdToDocIdNoThresh.xlsx'\n",
    "save_xls(outAddr+filename, topicIdToDocIdDfList, tabNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print (len(topicIdToDocIdDfList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topicIdToDocIdDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### What are the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "x = list(topicIdToTopWords.keys())\n",
    "x.sort()\n",
    "for i in x:\n",
    "    print (\"Topic #\", i, \": \", topicIdToTopWords[i])\n",
    "    print ()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Get document id to topic id dictionary of probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_topic = nmf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "docIdToTopicIdProb = {}\n",
    "for n in range(doc_topic.shape[0]): #for each document\n",
    "    docName = dataDf.loc[n]['name']\n",
    "    print (docName)\n",
    "    docIdToTopicIdProb[docName] = {}\n",
    "    topicIds = range(0, len(doc_topic[n]))\n",
    "    for topicId in topicIds:\n",
    "#         print (topicId)\n",
    "        docIdToTopicIdProb[docName][topicId] = doc_topic[n][topicId]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot the distribution, y axis number of documents, x importance to a document. will have as many distributions as n_topics - andys distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (not os.path.exists(outAddr+'AndysPlotsImpToADocVsNoDocsForEachTopic/')):\n",
    "    os.makedirs(outAddr+'AndysPlotsImpToADocVsNoDocsForEachTopic/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topicIds = sorted(list(topicIdToDocId.keys()))\n",
    "xs = []\n",
    "group_labels = []\n",
    "for topicId in topicIds:\n",
    "    print (topicId)\n",
    "#     if topicId < 1:\n",
    "#         continue\n",
    "    group_label = 'Topic '+ str(topicId) + ': ' + topicIdToTopWords[topicId]\n",
    "    x = []\n",
    "    for docId in docIdToTopicIdProb:\n",
    "        x.append(docIdToTopicIdProb[docId][topicId])\n",
    "    \n",
    "    xs.append(x)\n",
    "    group_labels.append(group_label)\n",
    "    \n",
    "    x = list(filter(lambda a: a > 0, x))\n",
    "    x = list(filter(lambda a: a >= 0.01, x)) #filter out importances of less than 0.02 for clarity \n",
    "\n",
    "    fig = sns.distplot(x, hist=True, kde=True, rug=False, bins=10)\n",
    "    fig.set_title(group_label)\n",
    "    fig = fig.get_figure()\n",
    "    fig.savefig(outAddr+'AndysPlotsImpToADocVsNoDocsForEachTopic/'+'topic'+str(topicId)+'.png')\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
